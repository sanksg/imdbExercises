{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import naive bayes and create a pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_searchcv import GridSearchCV as DaskGridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results Dict\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load IMDB (Original Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helpers' from 'C:\\\\Users\\\\sankalpg\\\\Desktop\\\\Learning\\\\notebooks\\\\imdbExercise\\\\helpers.py'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading IMDB dataset...\n",
      "Original dataframe shape:  (100000, 4)\n",
      "Original DF columns: Index(['type', 'review', 'label', 'file'], dtype='object')\n",
      "Original Train/Test split: train    75000\n",
      "test     25000\n",
      "Name: type, dtype: int64\n",
      "Dropping the file column...\n",
      "Dropping the unlabeled rows and splitting into train/test...\n",
      "X_train, y_train, shapes: (25000,) (25000,)\n",
      "y_train counts: neg    12500\n",
      "pos    12500\n",
      "Name: label, dtype: int64\n",
      "X_test, y_test, shapes: (25000,) (25000,)\n",
      "y_test counts: neg    12500\n",
      "pos    12500\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "path = '../kaggleData/imdb-review-dataset/imdb_master.csv'\n",
    "x_train, y_train, x_test, y_test = helpers.load_imdb(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n",
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print(type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000    neg\n",
       "25001    neg\n",
       "25002    neg\n",
       "25003    neg\n",
       "25004    neg\n",
       "25005    neg\n",
       "25006    neg\n",
       "25007    neg\n",
       "25008    neg\n",
       "25009    neg\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countPipe1 = Pipeline([\n",
    "    ('countVect', CountVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'countVect__binary':[False, True],\n",
    "          'countVect__ngram_range':[(1,1), (1,3)],\n",
    "          'countVect__stop_words':[None, stopwords.words('english')]\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing SkLearn gridsearch with CountVectorizer and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  22 out of  24 | elapsed:  3.9min remaining:   21.4s\n",
      "[Parallel(n_jobs=-1)]: Done  24 out of  24 | elapsed:  4.5min finished\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "cvSklearn = GridSearchCV(countPipe1, param_grid = params, n_jobs=-1, scoring='accuracy', verbose=5)\n",
    "cvSklearn.fit(x_train, y_train)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317.9818708896637\n"
     ]
    }
   ],
   "source": [
    "print(end-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'countVect__binary': True, 'countVect__ngram_range': (1, 3), 'countVect__stop_words': None}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.856 (+/-0.011) for {'countVect__binary': True, 'countVect__ngram_range': (1, 3), 'countVect__stop_words': None}\n",
      "0.841 (+/-0.005) for {'countVect__binary': False, 'countVect__ngram_range': (1, 3), 'countVect__stop_words': None}\n",
      "0.832 (+/-0.009) for {'countVect__binary': True, 'countVect__ngram_range': (1, 3), 'countVect__stop_words': True}\n",
      "0.808 (+/-0.010) for {'countVect__binary': False, 'countVect__ngram_range': (1, 3), 'countVect__stop_words': True}\n",
      "0.807 (+/-0.011) for {'countVect__binary': True, 'countVect__ngram_range': (1, 1), 'countVect__stop_words': True}\n",
      "0.802 (+/-0.013) for {'countVect__binary': True, 'countVect__ngram_range': (1, 1), 'countVect__stop_words': None}\n",
      "0.774 (+/-0.007) for {'countVect__binary': False, 'countVect__ngram_range': (1, 1), 'countVect__stop_words': True}\n",
      "0.771 (+/-0.009) for {'countVect__binary': False, 'countVect__ngram_range': (1, 1), 'countVect__stop_words': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helpers.print_gridSearch_report(cvSklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TfidfVectorizer - Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfPipe1 = Pipeline([\n",
    "    ('tfidfVect', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'tfidfVect__max_df':ss.uniform(0.1, 0.9),\n",
    "          'tfidfVect__binary':[False, True],\n",
    "          'tfidfVect__ngram_range':[(1,1), (1,3)],\n",
    "          'tfidfVect__stop_words':[None, stopwords.words('english')]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_cv = RandomizedSearchCV(tfidfPipe1, param_dist = params, scoring='accuracy', verbose=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 24 candidates, totalling 72 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done   2 tasks      | elapsed:   29.7s\n",
      "[Parallel(n_jobs=8)]: Done  56 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=8)]: Done  72 out of  72 | elapsed: 14.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=8)]: Done  72 out of  72 | elapsed: 14.6min finished\n",
      "C:\\Users\\sankalpg\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlpy\\lib\\site-packages\\sklearn\\utils\\validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfVect', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tru...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=8,\n",
       "       param_grid={'tfidfVect__max_df': [0.2, 0.5, 1.0], 'tfidfVect__binary': [False, True], 'tfidfVect__ngram_range': [(1, 1), (1, 3)], 'tfidfVect__stop_words': [None, ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', '...shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "tfidf_cv.fit(x_train, y_train)\n",
    "end=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'tfidfVect__binary': True, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.868 (+/-0.008) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.866 (+/-0.010) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.865 (+/-0.011) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.856 (+/-0.005) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.854 (+/-0.004) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.854 (+/-0.004) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.854 (+/-0.004) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.853 (+/-0.007) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.852 (+/-0.008) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': None}\n",
      "0.833 (+/-0.006) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.832 (+/-0.007) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.831 (+/-0.008) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 3), 'tfidfVect__stop_words': True}\n",
      "0.819 (+/-0.005) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.819 (+/-0.005) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.818 (+/-0.008) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "0.817 (+/-0.005) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.817 (+/-0.005) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "0.817 (+/-0.009) for {'tfidfVect__binary': True, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "0.792 (+/-0.010) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.791 (+/-0.011) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.5, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "0.791 (+/-0.010) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.790 (+/-0.010) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 1.0, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "0.788 (+/-0.011) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': True}\n",
      "0.788 (+/-0.010) for {'tfidfVect__binary': False, 'tfidfVect__max_df': 0.2, 'tfidfVect__ngram_range': (1, 1), 'tfidfVect__stop_words': None}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helpers.print_gridSearch_report(tfidf_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "317.9818708896637\n"
     ]
    }
   ],
   "source": [
    "print(end-st)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize on the ngram range and min_df as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidfPipe2 = Pipeline([\n",
    "    ('tfidfVect', TfidfVectorizer(max_df=0.2, binary=True)),\n",
    "    ('clf', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params2 = {'tfidfVect__ngram_range':[(1,3), (1,5), (1,7)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   9 | elapsed:  3.3min remaining:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of   9 | elapsed:  5.1min remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('tfidfVect', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.2, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True...rue,\n",
       "        vocabulary=None)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params=None, iid=True, n_jobs=-1,\n",
       "       param_grid={'tfidfVect__ngram_range': [(1, 3), (1, 5), (1, 7)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = time.time()\n",
    "tfidf_cv = GridSearchCV(tfidfPipe2, param_grid = params2, n_jobs=-1, scoring='accuracy', verbose=5)\n",
    "tfidf_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647.6303422451019\n"
     ]
    }
   ],
   "source": [
    "print(time.time()-st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'tfidfVect__ngram_range': (1, 5)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.870 (+/-0.009) for {'tfidfVect__ngram_range': (1, 5)}\n",
      "0.870 (+/-0.009) for {'tfidfVect__ngram_range': (1, 7)}\n",
      "0.868 (+/-0.008) for {'tfidfVect__ngram_range': (1, 3)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helpers.print_gridSearch_report(tfidf_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "logPipe1 = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('clf', LogisticRegression(solver='sag')),\n",
    "])\n",
    "\n",
    "\n",
    "params = {'vectorizer__max_df': [ 0.2, 0.5, 1.0],\n",
    "          'vectorizer__binary':[False, True],\n",
    "          'vectorizer__ngram_range':[(1,1), (1,3)],\n",
    "          'clf__C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 33.6min finished\n",
      "C:\\Users\\sankalpg\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlpy\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "       ... penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=20, n_jobs=-1,\n",
       "          param_distributions={'vectorizer__max_df': [0.2, 0.5, 1.0], 'vectorizer__binary': [False, True], 'vectorizer__ngram_range': [(1, 1), (1, 3)], 'clf__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cv = RandomizedSearchCV(logPipe1, param_distributions = params, n_iter=20, n_jobs=-1, scoring='accuracy', verbose=5)\n",
    "log_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 100}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.882 (+/-0.007) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 100}\n",
      "0.882 (+/-0.006) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': True, 'clf__C': 100}\n",
      "0.881 (+/-0.005) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': True, 'clf__C': 0.1}\n",
      "0.881 (+/-0.006) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.2, 'vectorizer__binary': True, 'clf__C': 0.01}\n",
      "0.876 (+/-0.007) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': True, 'clf__C': 0.01}\n",
      "0.875 (+/-0.007) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 0.01}\n",
      "0.874 (+/-0.007) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.2, 'vectorizer__binary': False, 'clf__C': 0.01}\n",
      "0.874 (+/-0.011) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': False, 'clf__C': 100}\n",
      "0.873 (+/-0.012) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': False, 'clf__C': 0.1}\n",
      "0.869 (+/-0.013) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 1.0, 'vectorizer__binary': False, 'clf__C': 100}\n",
      "0.864 (+/-0.006) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 0.2, 'vectorizer__binary': True, 'clf__C': 0.01}\n",
      "0.860 (+/-0.013) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 0.2, 'vectorizer__binary': True, 'clf__C': 0.1}\n",
      "0.859 (+/-0.013) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 1.0, 'vectorizer__binary': False, 'clf__C': 100}\n",
      "0.857 (+/-0.017) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 0.5, 'vectorizer__binary': False, 'clf__C': 10}\n",
      "0.857 (+/-0.002) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 0.5, 'vectorizer__binary': True, 'clf__C': 0.001}\n",
      "0.855 (+/-0.004) for {'vectorizer__ngram_range': (1, 3), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 0.001}\n",
      "0.851 (+/-0.017) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 10}\n",
      "0.851 (+/-0.016) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 100}\n",
      "0.847 (+/-0.016) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 0.2, 'vectorizer__binary': False, 'clf__C': 10}\n",
      "0.837 (+/-0.002) for {'vectorizer__ngram_range': (1, 1), 'vectorizer__max_df': 1.0, 'vectorizer__binary': True, 'clf__C': 0.001}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helpers.print_gridSearch_report(log_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### TFidf Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logPipe2 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('clf', LogisticRegression(solver='sag')),\n",
    "])\n",
    "\n",
    "\n",
    "params = {'vectorizer__max_df': ss.uniform(0.1, 1.0),\n",
    "          'vectorizer__binary':[True],\n",
    "          'vectorizer__ngram_range':[(1,3)],\n",
    "          'clf__C': [10, 100, 1000]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed: 62.4min finished\n",
      "C:\\Users\\sankalpg\\AppData\\Local\\Continuum\\anaconda3\\envs\\mlpy\\lib\\site-packages\\sklearn\\linear_model\\sag.py:326: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=None, error_score='raise',\n",
       "          estimator=Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=Tr... penalty='l2', random_state=None, solver='sag', tol=0.0001,\n",
       "          verbose=0, warm_start=False))]),\n",
       "          fit_params=None, iid=True, n_iter=20, n_jobs=-1,\n",
       "          param_distributions={'vectorizer__max_df': <scipy.stats._distn_infrastructure.rv_frozen object at 0x000001F7EDE90DD8>, 'vectorizer__binary': [True], 'vectorizer__ngram_range': [(1, 3)], 'clf__C': [10, 100, 1000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          return_train_score='warn', scoring='accuracy', verbose=5)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_cv = RandomizedSearchCV(logPipe2, param_distributions = params, n_iter=20, n_jobs=-1, scoring='accuracy', verbose=5)\n",
    "log_cv.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.2645730704175282, 'vectorizer__ngram_range': (1, 3)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.888 (+/-0.009) for {'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.2645730704175282, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.888 (+/-0.006) for {'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.43095227191752072, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.887 (+/-0.006) for {'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.46507593054654572, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.887 (+/-0.005) for {'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.85052342685369797, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.887 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.48210991720845253, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.887 (+/-0.005) for {'clf__C': 1000, 'vectorizer__binary': True, 'vectorizer__max_df': 0.6205307175120871, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.887 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.572328649368537, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 1.0452128114922339, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.94175307737746572, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 1.0339145387503448, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 1.0385913834416807, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.96124672906839725, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 1.0755520085045653, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.82347848826195502, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.84707894514364512, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.006) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.75807237807595795, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.886 (+/-0.005) for {'clf__C': 100, 'vectorizer__binary': True, 'vectorizer__max_df': 0.6651091118206397, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.884 (+/-0.005) for {'clf__C': 10, 'vectorizer__binary': True, 'vectorizer__max_df': 0.89498309318229985, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.884 (+/-0.005) for {'clf__C': 10, 'vectorizer__binary': True, 'vectorizer__max_df': 0.77026561052956277, 'vectorizer__ngram_range': (1, 3)}\n",
      "0.884 (+/-0.005) for {'clf__C': 10, 'vectorizer__binary': True, 'vectorizer__max_df': 0.95183515678726027, 'vectorizer__ngram_range': (1, 3)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "helpers.print_gridSearch_report(log_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logPipe3 = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(1,3), max_df=0.5, binary=True)),\n",
    "    ('clf', LogisticRegression(C=100)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vectorizer', TfidfVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=Tru...ty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logPipe3.fit(x_train, LabelEncoder().fit_transform(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.907280\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy: %f\" % logPipe3.score(x_test, LabelEncoder().fit_transform(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
